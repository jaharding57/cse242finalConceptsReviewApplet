const llms = [
  { q: "What are the five main components that affect LLM training?", a: "Architecture, training algorithm/loss, data, evaluation, and systems." },
  { q: "What is the core task of a language model?", a: "To model the probability distribution over sequences of tokens (e.g., p(x₁, ..., xₗ))." },
  { q: "What is an autoregressive language model?", a: "A model that generates text by predicting the next token based on previous ones using the chain rule of probability." },
  { q: "What are the steps in autoregressive language model inference?", a: "1. Tokenize input\n2. Forward pass\n3. Predict next-token probabilities\n4. Sample next token\n5. Detokenize output" },
  { q: "What loss function is used in training LMs?", a: "Cross-entropy loss, which maximizes the log-likelihood of correct token predictions." },
  { q: "Why not use words or characters as tokens?", a: "Words are too coarse (typos, rare words); characters are too long. Tokens strike a balance using frequent subword units." },
  { q: "What is Byte Pair Encoding (BPE)?", a: "A subword tokenization method that merges frequent token pairs to form a vocabulary of tokens." },
  { q: "Why is BPE effective?", a: "It handles typos, reduces vocabulary size, and compresses common patterns across words." },
  { q: "What is perplexity in LLMs?", a: "A metric that measures how well the model predicts the next token. Lower perplexity means better performance." },
  { q: "What does low perplexity indicate?", a: "The model is confident and accurate in its predictions; it's hesitating among fewer tokens." },
  { q: "Why is perplexity no longer widely used for academic benchmarks?", a: "It is narrow in scope and doesn't capture real-world LLM capabilities across tasks." },
  { q: "What is HELM?", a: "Holistic Evaluation of Language Models — it evaluates models across a range of tasks using standardized metrics." },
  { q: "What is MMLU?", a: "Massive Multitask Language Understanding — a benchmark that tests LLMs across academic and professional subjects." },
  { q: "What are evaluation challenges in LLMs?", a: "Prompt sensitivity, inconsistencies, and contamination from training data." },
  { q: "What is the main source of LLM pretraining data?", a: "Filtered internet-scale corpora like Common Crawl, cleaned and deduplicated." },
  { q: "What are some challenges with web data for LLMs?", a: "Boilerplate content, HTML artifacts, low quality text, and duplication." },
  { q: "What are major public datasets for LLMs?", a: "C4, The Pile, Dolma, and FineWeb." },
  { q: "What is the role of synthetic data in training LLMs?", a: "To augment real data when high-quality, human-written content is limited or expensive." },
  { q: "Why is there secrecy around LLM training data?", a: "Due to competitive advantage concerns and copyright/legal risks." },
  { q: "What are scaling laws in LLMs?", a: "Empirical laws that show performance improves with more data and larger models." },
  { q: "What is the Chinchilla scaling rule?", a: "Train with ~20 tokens per model parameter for optimal compute efficiency." },
  { q: "How do scaling laws influence model design?", a: "They guide decisions about model size, data volume, and training duration." },
  { q: "What is the 'bitter lesson' in AI?", a: "Scale and computation outperform human-designed heuristics in the long run." },
  { q: "Why are Transformers the dominant architecture in LLMs?", a: "They scale better and perform better than LSTMs or RNNs." },
  { q: "What is the role of FLOPs in LLM training?", a: "FLOPs measure total training compute, which correlates with model quality." },
  { q: "How long and costly is training a SOTA model like LLaMA 3?", a: "About $75M over 70 days using 16,000 GPUs." },
  { q: "What is 'alignment' in LLMs?", a: "Post-training process that tunes the model to follow human intent and avoid harmful outputs." },
  { q: "Why is post-training necessary?", a: "Language modeling alone doesn't produce helpful or safe behavior." },
  { q: "How is post-training done?", a: "Using small curated datasets and methods like supervised fine-tuning or RLHF." },
  { q: "What is nanoGPT?", a: "A lightweight educational implementation of a GPT-style model." }
];