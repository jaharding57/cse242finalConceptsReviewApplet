const diffusion = [
  { q: "What are the two main processes in diffusion models?", a: "Forward diffusion (adds noise) and reverse denoising (removes noise to generate data)." },
  { q: "What does the forward diffusion process do?", a: "Gradually adds Gaussian noise to the input until it becomes indistinguishable from pure noise." },
  { q: "What is the reverse diffusion process?", a: "It learns to denoise the input step-by-step to generate realistic data samples." },
  { q: "How are diffusion models trained?", a: "They are trained to maximize the Evidence Lower Bound (ELBO), a tractable proxy for log-likelihood." },
  { q: "Why is ELBO used instead of exact likelihood?", a: "Because exact likelihood is intractable for high-dimensional data like images." },
  { q: "What is the simplified training objective used in practice?", a: "A noise prediction loss (like MSE) that correlates well with ELBO." },
  { q: "How does sampling work in diffusion models?", a: "The model starts with random noise and iteratively denoises it to produce data." },
  { q: "What is classifier-free guidance?", a: "A technique that uses both conditioned and unconditioned model outputs to steer generation without needing a separate classifier." },
  { q: "How is classifier-free guidance implemented?", a: "By randomly dropping the condition label during training and combining outputs at inference." },
  { q: "What is the benefit of classifier-free guidance?", a: "It improves generation quality without requiring an external classifier model." },
  { q: "What is text-conditioned diffusion?", a: "A form of diffusion where the model is conditioned on text prompts instead of class labels." },
  { q: "How is text information injected into the model?", a: "Via cross-attention mechanisms applied in the U-Net layers." },
  { q: "What is Latent Diffusion?", a: "A method where diffusion operates in a lower-dimensional latent space, improving efficiency and resolution." },
  { q: "Why use latent space instead of pixel space?", a: "It reduces computational cost and improves quality at higher resolutions." },
  { q: "What does the encoder/decoder do in latent diffusion?", a: "The encoder compresses images to latents; the decoder reconstructs them post-generation." },
  { q: "What is ControlNet?", a: "A wrapper around a diffusion model that enables conditional control like edge maps or segmentation." },
  { q: "What tasks is ControlNet good for?", a: "Pix2pix-style tasks, like image translation or structured input conditioning." },
  { q: "What is Prompt-to-Prompt image editing?", a: "It modifies an image based on changes in the prompt using cross-attention alignment." },
  { q: "When is Prompt-to-Prompt editing most effective?", a: "When editing an image that was originally generated by a diffusion model." },
  { q: "What is InstructPix2Pix?", a: "A fine-tuned diffusion model that follows natural language editing instructions for images." },
  { q: "What dataset does InstructPix2Pix use?", a: "Synthetic instruction-image pairs created from prompt editing." },
  { q: "What is DiT (Diffusion Transformer)?", a: "A scalable diffusion model that uses a Vision Transformer instead of a U-Net." },
  { q: "How does DiT differ from traditional diffusion models?", a: "It tokenizes image patches and processes them with transformer layers instead of convolutional ones." },
  { q: "What does scaling law tell us about DiT?", a: "That more compute during training improves Fréchet Inception Distance (FID) scores." },
  { q: "What is patchify in DiT?", a: "The operation of converting images into fixed-size patches (e.g., 8x8) for transformer input." },
  { q: "What are DiT model sizes?", a: "Variants include DiT-S, DiT-B, DiT-L, and DiT-XL — small to extra-large." }
];