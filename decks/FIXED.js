const dl_flashcards = [
  {
    "q": "What is the purpose of activation functions in neural networks?",
    "a": "They introduce non-linearities, enabling networks to model complex functions. Examples include Sigmoid, tanh, and ReLU."
  },
  {
    "q": "How does a multilayer perceptron (MLP) learn?",
    "a": "It uses a feed-forward architecture and learns weights through backpropagation and gradient descent using a loss function like cross-entropy."
  },
  {
    "q": "What architecture is commonly used for image classification tasks?",
    "a": "Convolutional Neural Networks (CNNs), which use convolutional and pooling layers to learn spatial hierarchies."
  },
  {
    "q": "What is dropout in deep learning?",
    "a": "A regularization technique that randomly disables neurons during training to prevent overfitting and encourage robustness."
  },
  {
    "q": "What is a vanishing gradient and how does ReLU help?",
    "a": "Vanishing gradients occur when derivatives shrink during backpropagation. ReLU mitigates this by not saturating for positive inputs."
  }
];